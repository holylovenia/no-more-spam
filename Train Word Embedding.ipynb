{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/holy/anaconda3/envs/nlp_text/lib/python3.7/site-packages/sklearn/utils/__init__.py:4: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Sequence\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(contents):\n",
    "    stop_words = list(stopwords.words('english'))\n",
    "    for w in stop_words:\n",
    "        contents = contents.replace(w, '')\n",
    "        \n",
    "    return contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize and remove unnecessary characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unnecessary_characters(contents):\n",
    "    contents = contents.replace('\\n', ' ')\n",
    "    contents = contents.replace('..', '')\n",
    "    contents = contents.replace('--', '')\n",
    "    contents = contents.replace('==', '')\n",
    "    contents = contents.replace('///', '')\n",
    "    contents = contents.replace('\\\\\\\\', '')\n",
    "    contents = ' '.join(contents.split())\n",
    "    contents = contents.strip().lower()\n",
    "    \n",
    "#     contents = remove_stop_words(contents)\n",
    "    tokenizer = RegexpTokenizer('[A-Za-z0-9\\@\\.\\&\\/\\:\\$\\-\\_]+')\n",
    "    tokens = tokenizer.tokenize(contents)\n",
    "    \n",
    "    tokens = ' '.join( [i for i in tokens if len(i) > 1])\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_email(content):\n",
    "    pattern = re.compile('[\\w\\/\\.\\-]+\\@[\\w\\/\\.\\-]+\\.[\\w]+')\n",
    "    replaced_content = re.sub(pattern, 'this_is_email', content)\n",
    "    return replaced_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_link(content):\n",
    "    pattern = re.compile('(http[s]?:\\/\\/|www\\.)?[\\w\\/\\.\\-]+\\.(com|html|php)([\\/][\\w\\/\\.\\-]*)*')\n",
    "    replaced_content = re.sub(pattern, 'this_is_link', content)\n",
    "    return replaced_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read CSV data for train data and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('dataset/train_data.csv')\n",
    "test_data = pd.read_csv('dataset/test_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize remove unnecessary characters for train data and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    daytips poem-a-day: 09/13/02 sponsor child tod...\n",
       "1    jody sent you messagejody sent you message. tr...\n",
       "2    re: tricky perl question ascending orderjozsi ...\n",
       "3    this_is_email to unsubscribe email to this_is_...\n",
       "4    re: re moment of silence for the first amendme...\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preproc_train_data = train_data['content'].copy().apply(remove_unnecessary_characters).apply(replace_email).apply(replace_link)\n",
    "\n",
    "print('Train data')\n",
    "preproc_train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    re: acroread not seeing printerson thu 2010-04...\n",
       "1    america great misleaderurl: this_is_link 86740...\n",
       "2    lowestprices guaranteed on flea and tick meds ...\n",
       "3    re: problems with apt-get -f install once upon...\n",
       "4    ack apt-get still failing for me stumped. rh8 ...\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preproc_test_data = test_data['content'].copy().apply(remove_unnecessary_characters).apply(replace_email).apply(replace_link)\n",
    "\n",
    "print('Test data')\n",
    "preproc_test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in preproc_train_data.iteritems():\n",
    "    corpus.append(row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in preproc_test_data.iteritems():\n",
    "    corpus.append(row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cbow = gensim.models.Word2Vec(\n",
    "    corpus,\n",
    "    size=50,\n",
    "    sg=0,\n",
    "    window=2,\n",
    "    min_count=5,\n",
    "    workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(79087484, 384610850)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cbow.train(corpus, total_examples=len(corpus), epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sg = gensim.models.Word2Vec(\n",
    "    corpus,\n",
    "    size=50,\n",
    "    sg=1,\n",
    "    window=2,\n",
    "    min_count=5,\n",
    "    workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(79095938, 384610850)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sg.train(corpus, total_examples=len(corpus), epochs=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
